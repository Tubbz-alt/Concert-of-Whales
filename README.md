Concert of Whales
=========================

Generative Models to Synthesize Audio Waveforms

Deep learning has made tremendous leaps and bounds in the field of speech synthesis. While more classical TTS (text-to-speech) pipelines relied on domain expertise to develop different models of linguistic features, vocoders, etc., deep learning-based approaches can be fully end-to-end, thereby preventing errors from compounding, and allowing the neural network to learn the features of importance that would otherwise be hand-engineered. Several different methodologies have been used to tackle this particular problem: one, [Tacotron](https://arxiv.org/pdf/1703.10135.pdf), uses a seq2seq model with attention to encode an input sequence of characters and decode it into raw, linear spectrogram frames. The Griffin-Lim algorithm can then be used to synthesize an audio waveform from the spectal magnitudes. Two other popular approaches are Wavenet and [SampleRNN](https://arxiv.org/pdf/1612.07837.pdf), autoregressive models that generate audio directly on a sample-by-sample basis. While Wavenet relies on dilated convolutional layers, SampleRNN uses a hierarchy of RNN modules to model long-term dependencies at different temporal scales.

The goal of this application was to synthesize a 2-second-long audio waveform of a right whale upcall via generative modeling. Due to the computationally-intensive requirements of sample-level generation, a model based on Tacotron that predicted spectrogram images (of much lower dimensionality than raw audio waveforms) was taken up first. 

The original [Tacotron paper](https://arxiv.org/pdf/1703.10135.pdf) describes a CBHG module (1-D convolution bank + highway network + Bidirectional GRU) capable of extracting excellent representations from sequences by convolving the sequence first with a bank of 1-D convolutional filters to extract local information, passing it through a highway network to extract higher-level features, and finally passing the sequence through a Bidirectional GRU to learn long-term dependencies in the forward and backward directions. In the Tacotron model, an encoder uses this CBHG module to extract a sequential representation of input text, which the attention-based
decoder uses to create a sequence of spectrogram frames that can be used to synthesize the correspnoding waveform. For simplicity, the decoder targets are 80-band mel spectrograms (a compressed representation that can be used by 
a post-processing-net later on in the model to synthesize raw spectrograms). This post-processing-net is once again
composed of a CBHG module, which learns to predict spectral magnitudes on a linear frequency scale (due to the use
of the Griffin-Lim algorithm to create waveforms).

A representative sample of an audio waveform synthesized from a spectrogram generated by the “Tacotron” model can be found here: 

There are clear audio artifacts in the audio waveform due to either the Griffin-Lim algorithm or the need for a more optimized model architecture/hyperparameters. 

In order to obtain more realistic-sounding results, the next model to be investigated was [SampleRNN](https://arxiv.org/pdf/1612.07837.pdf). A Three-Tier SampleRNN was implemented, in which each of the three modules in the hierarchy conditions the one below it so that the lowest module outputs sample-by-sample predictions. Concretely, the highest level module processes the previous 8 samples of the time series  using an RNN and passes a conditioning vector to the second-highest-level module, which in turn processes the previous 2 samples of the time series using an RNN and passes a conditioning vector to the lowest-level module. This module processes only the previous sample of the time series and outputs a q=256-way softmax over quantized values of the audio waveform. (As an additional design choice, which was proven to improve results, the lowest module passes the previous (quantized) value of the audio clip time series through an embedding layer, which maps each of the quantized values to a real-valued vector embedding. Further, to speed up training, a Multilayer Perceptron is used in the lowest-level module instead of an RNN to yield the final (quantized) prediction for the next sample). 

After training, the SampleRNN is tasked with synthesizing a new 2-second long right whale up call sound. A representative sample can be found here: 

In order to demonstrate the high degree of realism attained by SampleRNN with only a few epochs of training, an application was developed in which four whales are drawn to a canvas, three of which make “real” right whale upcall sounds from the training set, and the fourth makes the “fake” upcall sound created by the neural network. The task of the user is to identify the whale that made the "fake" right whale upcall sound.

The application begins by displaying the four baby whales on the canvas, each of which is a different size and swims in a different direction. 

![babywhales](https://github.com/cchinchristopherj/Concert-of-Whales/blob/cchinchristopherj-patch-1/Images/babywhales.png)

The whales will quickly grow to full size, as indicated by the progress bar, after which they can make their upcall sounds to each other. 

![finalscreen](https://github.com/cchinchristopherj/Concert-of-Whales/blob/cchinchristopherj-patch-1/Images/finalscreen.png)

In nature, these upcalls are made between whales as a greeting to other whales close by. To replicate this in the application, the upcall sounds associated with each individual whale are played when the center points of their shapes are close enough (in terms of Euclidean distance) to each other, thereby creating a chorus of sound when the whales randomly cluster together. The user also has the option of clicking the name of a whale to play their corresponding up call sound, in order to better inform their decision of which whale is the “fake.” 

Listening to each of the upcall sounds in turn reveals that the “fake” sound generated by SampleRNN is nearly indistinguishable from the sounds from the actual training set. With more training and computational resources, the SampleRNN could achieve even more realistic results. 
